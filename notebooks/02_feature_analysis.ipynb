{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Initializing feature engineering...\n",
      "\n",
      "Verifying feature distributions...\n",
      "\n",
      "Creating temporal features...\n",
      "Creating symbol features...\n",
      "Creating regime features...\n",
      "\n",
      "Handling missing values...\n",
      "\n",
      "Checking for multicollinearity...\n",
      "\n",
      "=== Feature Engineering Summary ===\n",
      "Original features: 79\n",
      "Engineered features: -9\n",
      "High correlation pairs: 0\n",
      "\n",
      "Saving processed dataset...\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering for Jane Street Market Prediction\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Custom utilities\n",
    "from src.data.data_loader import JaneStreetDataLoader\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering pipeline for market prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df\n",
    "        self.feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "        \n",
    "    def verify_feature_distributions(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Check feature scales and distributions\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        for col in self.feature_cols:\n",
    "            if self.df[col].null_count() / len(self.df) < 0.5:  # Only analyze features with <50% missing\n",
    "                stats[col] = {\n",
    "                    'mean': float(self.df[col].mean()),\n",
    "                    'std': float(self.df[col].std()),\n",
    "                    'min': float(self.df[col].min()),\n",
    "                    'max': float(self.df[col].max()),\n",
    "                    'null_pct': float(self.df[col].null_count() / len(self.df))\n",
    "                }\n",
    "        return stats\n",
    "    \n",
    "    def create_temporal_features(self, \n",
    "                               lags: List[int] = [1, 2, 3, 5, 10],\n",
    "                               windows: List[int] = [5, 10, 20]) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create time-based features\n",
    "        \"\"\"\n",
    "        df = self.df.sort(['symbol_id', 'date_id', 'time_id'])\n",
    "        \n",
    "        # 1. Lag features\n",
    "        for lag in lags:\n",
    "            # Target lags\n",
    "            df = df.with_columns([\n",
    "                pl.col('responder_6')\n",
    "                  .shift(lag)\n",
    "                  .over('symbol_id')\n",
    "                  .alias(f'target_lag_{lag}')\n",
    "            ])\n",
    "            \n",
    "            # Feature lags for important features\n",
    "            for feat in self.get_important_features():\n",
    "                df = df.with_columns([\n",
    "                    pl.col(feat)\n",
    "                      .shift(lag)\n",
    "                      .over('symbol_id')\n",
    "                      .alias(f'{feat}_lag_{lag}')\n",
    "                ])\n",
    "        \n",
    "        # 2. Rolling statistics\n",
    "        for window in windows:\n",
    "            df = df.with_columns([\n",
    "                pl.col('responder_6')\n",
    "                  .rolling_mean(window)\n",
    "                  .over('symbol_id')\n",
    "                  .alias(f'return_ma_{window}'),\n",
    "                  \n",
    "                pl.col('responder_6')\n",
    "                  .rolling_std(window)\n",
    "                  .over('symbol_id')\n",
    "                  .alias(f'return_vol_{window}')\n",
    "            ])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_symbol_features(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create symbol-specific features\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # 1. Symbol level statistics\n",
    "        symbol_stats = (df.group_by('symbol_id')\n",
    "                         .agg([\n",
    "                             pl.col('responder_6').mean().alias('symbol_mean_return'),\n",
    "                             pl.col('responder_6').std().alias('symbol_volatility'),\n",
    "                             pl.len().alias('symbol_activity')\n",
    "                         ]))\n",
    "        \n",
    "        # Join back to main dataset\n",
    "        df = df.join(symbol_stats, on='symbol_id')\n",
    "        \n",
    "        # 2. Symbol regime indicators\n",
    "        df = df.with_columns([\n",
    "            (pl.col('symbol_volatility') > \n",
    "             pl.col('symbol_volatility').mean() + pl.col('symbol_volatility').std())\n",
    "            .cast(pl.Int32)\n",
    "            .alias('high_vol_symbol'),\n",
    "            \n",
    "            (pl.col('symbol_activity') > \n",
    "             pl.col('symbol_activity').mean() + pl.col('symbol_activity').std())\n",
    "            .cast(pl.Int32)\n",
    "            .alias('high_activity_symbol')\n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_regime_features(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create market regime indicators\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # 1. Calculate daily volatility\n",
    "        daily_stats = (df.group_by('date_id')\n",
    "                        .agg([\n",
    "                            pl.col('responder_6').std().alias('daily_vol'),\n",
    "                            pl.col('responder_6').mean().alias('daily_return'),\n",
    "                            pl.len().alias('daily_trades')\n",
    "                        ]))\n",
    "        \n",
    "        vol_mean = float(daily_stats['daily_vol'].mean())\n",
    "        vol_std = float(daily_stats['daily_vol'].std())\n",
    "        \n",
    "        # 2. Create regime indicators\n",
    "        daily_stats = daily_stats.with_columns([\n",
    "            pl.when(pl.col('daily_vol') > vol_mean + vol_std)\n",
    "              .then(pl.lit('high'))\n",
    "              .when(pl.col('daily_vol') < vol_mean - vol_std)\n",
    "              .then(pl.lit('low'))\n",
    "              .otherwise(pl.lit('normal'))\n",
    "              .alias('vol_regime')\n",
    "        ])\n",
    "        \n",
    "        # 3. Join back to main dataset\n",
    "        df = df.join(daily_stats.select(['date_id', 'vol_regime']), on='date_id')\n",
    "        \n",
    "        # 4. Create dummy variables for regimes\n",
    "        df = df.with_columns([\n",
    "            (pl.col('vol_regime') == 'high').cast(pl.Int32).alias('high_vol_regime'),\n",
    "            (pl.col('vol_regime') == 'low').cast(pl.Int32).alias('low_vol_regime')\n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply missing value strategy\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # 1. Identify features by missing value levels\n",
    "        feature_missing = {col: df[col].null_count() / len(df) \n",
    "                         for col in self.feature_cols}\n",
    "        \n",
    "        high_missing = [col for col, pct in feature_missing.items() \n",
    "                       if pct > 0.5]\n",
    "        moderate_missing = [col for col, pct in feature_missing.items() \n",
    "                          if 0.1 < pct <= 0.5]\n",
    "        low_missing = [col for col, pct in feature_missing.items() \n",
    "                      if 0 < pct <= 0.1]\n",
    "        \n",
    "        # 2. Drop high missing features\n",
    "        df = df.drop(high_missing)\n",
    "        \n",
    "        # 3. Forward fill moderate missing (within symbols)\n",
    "        for col in moderate_missing:\n",
    "            df = df.with_columns([\n",
    "                pl.col(col)\n",
    "                  .forward_fill()\n",
    "                  .over('symbol_id')\n",
    "                  .alias(col)\n",
    "            ])\n",
    "        \n",
    "        # 4. Interpolate low missing\n",
    "        for col in low_missing:\n",
    "            df = df.with_columns([\n",
    "                pl.col(col)\n",
    "                  .interpolate()\n",
    "                  .alias(col)\n",
    "            ])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_important_features(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identify important features based on correlation with target\n",
    "        \"\"\"\n",
    "        important_features = []\n",
    "        for col in self.feature_cols:\n",
    "            if self.df[col].null_count() / len(self.df) < 0.5:\n",
    "                corr = self.df.select(pl.corr(col, 'responder_6')).item()\n",
    "                if abs(corr) > 0.1:  # Arbitrary threshold\n",
    "                    important_features.append(col)\n",
    "        return important_features\n",
    "    \n",
    "    def check_multicollinearity(self, threshold: float = 0.8) -> Dict:\n",
    "        \"\"\"\n",
    "        Check for high correlations between features\n",
    "        \"\"\"\n",
    "        high_corr_pairs = {}\n",
    "        features = self.get_important_features()\n",
    "        \n",
    "        for i, feat1 in enumerate(features):\n",
    "            for feat2 in features[i+1:]:\n",
    "                corr = abs(self.df.select(pl.corr(feat1, feat2)).item())\n",
    "                if corr > threshold:\n",
    "                    high_corr_pairs[(feat1, feat2)] = corr\n",
    "        \n",
    "        return high_corr_pairs\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "loader = JaneStreetDataLoader()\n",
    "sample_data = loader.load_training_sample(n_partitions=2)\n",
    "\n",
    "# Create feature engineer\n",
    "print(\"\\nInitializing feature engineering...\")\n",
    "engineer = FeatureEngineer(sample_data)\n",
    "\n",
    "# 1. Verify feature distributions\n",
    "print(\"\\nVerifying feature distributions...\")\n",
    "feature_stats = engineer.verify_feature_distributions()\n",
    "\n",
    "# 2. Create features\n",
    "print(\"\\nCreating temporal features...\")\n",
    "df = engineer.create_temporal_features()\n",
    "\n",
    "print(\"Creating symbol features...\")\n",
    "df = engineer.create_symbol_features()\n",
    "\n",
    "print(\"Creating regime features...\")\n",
    "df = engineer.create_regime_features()\n",
    "\n",
    "# 3. Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "df = engineer.handle_missing_values()\n",
    "\n",
    "# 4. Check for multicollinearity\n",
    "print(\"\\nChecking for multicollinearity...\")\n",
    "high_corr_features = engineer.check_multicollinearity()\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== Feature Engineering Summary ===\")\n",
    "print(f\"Original features: {len(engineer.feature_cols)}\")\n",
    "print(f\"Engineered features: {len(df.columns) - len(sample_data.columns)}\")\n",
    "print(f\"High correlation pairs: {len(high_corr_features)}\")\n",
    "\n",
    "# Save processed dataset\n",
    "print(\"\\nSaving processed dataset...\")\n",
    "df.write_parquet(\"../data/processed/engineered_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Initializing feature engineering...\n",
      "\n",
      "Stage 1: Creating base features...\n",
      "Error creating base features for window 5: argument 'interpolation': 'float' object cannot be converted to 'PyString'\n",
      "Error creating base features for window 10: argument 'interpolation': 'float' object cannot be converted to 'PyString'\n",
      "Error creating base features for window 20: argument 'interpolation': 'float' object cannot be converted to 'PyString'\n",
      "\n",
      "Stage 2: Detecting market regimes...\n",
      "\n",
      "Stage 3: Creating regime-aware features...\n",
      "Error creating regime features: sub operation not supported for dtypes `str` and `str`\n",
      "\n",
      "Stage 4: Creating symbol-specific features...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MarketRegime:\n",
    "    \"\"\"Market regime configuration\"\"\"\n",
    "    name: str\n",
    "    volatility_threshold: float\n",
    "    correlation_threshold: float\n",
    "\n",
    "class EnhancedFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering with regime awareness and robust calculations\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        \"\"\"Initialize with dataframe and detect feature columns\"\"\"\n",
    "        self.df = df\n",
    "        self.feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "        self.regimes = None\n",
    "        self._set_dynamic_thresholds()\n",
    "    \n",
    "    def _set_dynamic_thresholds(self) -> None:\n",
    "        \"\"\"Set regime thresholds using historical data distribution\"\"\"\n",
    "        try:\n",
    "            # Calculate daily volatility\n",
    "            vol_stats = (self.df\n",
    "                        .group_by('date_id')\n",
    "                        .agg(pl.col('responder_6').std().alias('daily_vol')))\n",
    "            \n",
    "            # Get distribution quantiles\n",
    "            vol_quantiles = vol_stats.select([\n",
    "                pl.col('daily_vol').quantile(0.33).alias('low_vol'),\n",
    "                pl.col('daily_vol').quantile(0.67).alias('high_vol')\n",
    "            ])\n",
    "            \n",
    "            # Set thresholds\n",
    "            low_vol = float(vol_quantiles.get_column('low_vol')[0])\n",
    "            high_vol = float(vol_quantiles.get_column('high_vol')[0])\n",
    "            \n",
    "            self.regimes = {\n",
    "                'low_vol': MarketRegime('low_volatility', low_vol, 0.3),\n",
    "                'normal': MarketRegime('normal', high_vol, 0.5),\n",
    "                'high_vol': MarketRegime('high_volatility', float('inf'), 0.7)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting thresholds: {e}\")\n",
    "            # Set fallback thresholds\n",
    "            self.regimes = {\n",
    "                'low_vol': MarketRegime('low_volatility', 0.5, 0.3),\n",
    "                'normal': MarketRegime('normal', 1.0, 0.5),\n",
    "                'high_vol': MarketRegime('high_volatility', float('inf'), 0.7)\n",
    "            }\n",
    "    \n",
    "    def _validate_feature_set(self, df: pl.DataFrame, stage: str) -> None:\n",
    "        \"\"\"Validate feature quality after each stage\"\"\"\n",
    "        null_counts = df.null_count()\n",
    "        high_nulls = []\n",
    "        for col in df.columns:\n",
    "            null_count = null_counts.get_column(col)[0]\n",
    "            if null_count/len(df) > 0.1:\n",
    "                high_nulls.append((col, null_count))\n",
    "        \n",
    "        if high_nulls:\n",
    "            print(f\"\\nWarning: High null counts after {stage}:\")\n",
    "            for col, count in high_nulls:\n",
    "                print(f\"- {col}: {count} nulls ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    def create_base_features(self, \n",
    "                        windows: List[int] = [5, 10, 20],\n",
    "                        min_window_fraction: float = 0.5) -> pl.DataFrame:\n",
    "        \"\"\"Create foundational technical indicators\"\"\"\n",
    "        df = self.df.sort(['symbol_id', 'date_id', 'time_id'])\n",
    "        \n",
    "        for window in windows:\n",
    "            min_periods = max(int(window * min_window_fraction), 1)\n",
    "            \n",
    "            try:\n",
    "                # Fix: Use simpler rolling operations first\n",
    "                df = df.with_columns([\n",
    "                    # Price changes\n",
    "                    pl.col('responder_6')\n",
    "                    .diff()\n",
    "                    .over(['symbol_id'])\n",
    "                    .alias(f'price_change_{window}'),\n",
    "                    \n",
    "                    # Simple moving average\n",
    "                    pl.col('responder_6')\n",
    "                    .rolling_mean(window, min_periods=min_periods)\n",
    "                    .over(['symbol_id'])\n",
    "                    .alias(f'sma_{window}'),\n",
    "                    \n",
    "                    # Volatility\n",
    "                    pl.col('responder_6')\n",
    "                    .rolling_std(window, min_periods=min_periods)\n",
    "                    .over(['symbol_id'])\n",
    "                    .alias(f'volatility_{window}')\n",
    "                ])\n",
    "                \n",
    "                # Add lag features\n",
    "                df = df.with_columns([\n",
    "                    pl.col('responder_6')\n",
    "                    .shift(i)\n",
    "                    .over(['symbol_id'])\n",
    "                    .alias(f'lag_{i}')\n",
    "                    for i in range(1, 6)\n",
    "                ])\n",
    "                \n",
    "                # Calculate momentum using the SMA\n",
    "                df = df.with_columns([\n",
    "                    (pl.col('responder_6') - pl.col(f'sma_{window}'))\n",
    "                    .alias(f'momentum_{window}')\n",
    "                ])\n",
    "                \n",
    "                # Calculate returns safely\n",
    "                df = df.with_columns([\n",
    "                    (pl.col('responder_6') / \n",
    "                    pl.col('responder_6').shift(window)\n",
    "                    .over(['symbol_id'])\n",
    "                    .fill_null(pl.col('responder_6')) - 1)\n",
    "                    .alias(f'return_{window}')\n",
    "                ])\n",
    "                \n",
    "                self._validate_feature_set(df, f\"base features window {window}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating base features for window {window}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def detect_regimes(self, \n",
    "                  df: pl.DataFrame, \n",
    "                  window: int = 20,\n",
    "                  min_window_fraction: float = 0.5) -> pl.DataFrame:\n",
    "        \"\"\"Detect market regimes using multiple indicators\"\"\"\n",
    "        try:\n",
    "            min_periods = max(int(window * min_window_fraction), 1)\n",
    "            \n",
    "            # Calculate market metrics\n",
    "            market_metrics = (df.group_by(['date_id'])  # Removed time_id for stability\n",
    "                            .agg([\n",
    "                                pl.col('responder_6').std().alias('market_vol'),\n",
    "                                pl.col('responder_6').mean().alias('market_return'),\n",
    "                                pl.col('responder_6').count().alias('active_symbols')\n",
    "                            ]))\n",
    "            \n",
    "            # Add rolling metrics\n",
    "            market_metrics = market_metrics.with_columns([\n",
    "                pl.col('market_vol')\n",
    "                .rolling_mean(window, min_periods=min_periods)\n",
    "                .alias('rolling_vol')\n",
    "            ])\n",
    "            \n",
    "            # Join and detect regimes\n",
    "            df = df.join(market_metrics, on=['date_id'])\n",
    "            \n",
    "            # Create regime column\n",
    "            df = df.with_columns([\n",
    "                pl.when(pl.col('rolling_vol') > self.regimes['normal'].volatility_threshold)\n",
    "                .then(pl.lit('high_volatility'))\n",
    "                .when(pl.col('rolling_vol') < self.regimes['low_vol'].volatility_threshold)\n",
    "                .then(pl.lit('low_volatility'))\n",
    "                .otherwise(pl.lit('normal'))\n",
    "                .alias('volatility_regime')\n",
    "            ])\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in regime detection: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return df.with_columns(pl.lit('normal').alias('volatility_regime'))\n",
    "    \n",
    "    def create_regime_features(self, \n",
    "                         df: pl.DataFrame, \n",
    "                         windows: List[int] = [5, 10, 20]) -> pl.DataFrame:\n",
    "        \"\"\"Create regime-aware and adaptive features\"\"\"\n",
    "        try:\n",
    "            # Create regime indicators first\n",
    "            df = df.with_columns([\n",
    "                (pl.col('volatility_regime') == 'high_volatility')\n",
    "                .cast(pl.Int32)\n",
    "                .alias('high_vol_regime'),\n",
    "                (pl.col('volatility_regime') == 'low_volatility')\n",
    "                .cast(pl.Int32)\n",
    "                .alias('low_vol_regime')\n",
    "            ])\n",
    "            \n",
    "            for window in windows:\n",
    "                # Create regime-weighted features\n",
    "                df = df.with_columns([\n",
    "                    # Regime-weighted momentum\n",
    "                    pl.col(f'momentum_{window}')\n",
    "                    .mul(pl.when(pl.col('high_vol_regime') == 1)\n",
    "                            .then(0.7)\n",
    "                            .when(pl.col('low_vol_regime') == 1)\n",
    "                            .then(1.3)\n",
    "                            .otherwise(1.0))\n",
    "                    .alias(f'regime_momentum_{window}'),\n",
    "                    \n",
    "                    # Excess returns\n",
    "                    (pl.col(f'return_{window}') - \n",
    "                    pl.col(f'return_{window}').mean().over('date_id'))\n",
    "                    .alias(f'excess_return_{window}')\n",
    "                ])\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating regime features: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return df\n",
    "    \n",
    "    def create_symbol_features(self, \n",
    "                         df: pl.DataFrame,\n",
    "                         window: int = 20,\n",
    "                         min_periods: int = 10) -> pl.DataFrame:\n",
    "        \"\"\"Create symbol-specific features\"\"\"\n",
    "        try:\n",
    "            # Basic symbol metrics\n",
    "            symbol_metrics = (df.group_by('symbol_id')\n",
    "                            .agg([\n",
    "                                pl.col('responder_6').std().alias('symbol_vol'),\n",
    "                                pl.col('responder_6').mean().alias('symbol_return'),\n",
    "                                pl.col('responder_6').count().alias('symbol_liquidity')\n",
    "                            ]))\n",
    "            \n",
    "            # Join and create relative metrics\n",
    "            df = df.join(symbol_metrics, on='symbol_id')\n",
    "            \n",
    "            # Add rankings\n",
    "            df = df.with_columns([\n",
    "                pl.col('symbol_vol')\n",
    "                .rank()\n",
    "                .over('date_id')\n",
    "                .alias('vol_rank'),\n",
    "                pl.col('symbol_liquidity')\n",
    "                .rank()\n",
    "                .over('date_id')\n",
    "                .alias('liquidity_rank')\n",
    "            ])\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating symbol features: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return df\n",
    "    \n",
    "    def analyze_importance(self, \n",
    "                         df: pl.DataFrame, \n",
    "                         features: List[str],\n",
    "                         min_samples: int = 100) -> Dict:\n",
    "        \"\"\"Analyze feature importance with robust statistics\"\"\"\n",
    "        importance_stats = {}\n",
    "        \n",
    "        for feature in features:\n",
    "            try:\n",
    "                # Filter valid data using null check\n",
    "                valid_data = df.filter(\n",
    "                    ~pl.col(feature).is_null() & ~pl.col('responder_6').is_null()\n",
    "                )\n",
    "                \n",
    "                if len(valid_data) < min_samples:\n",
    "                    print(f\"Warning: Insufficient samples for {feature}\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate correlations\n",
    "                overall_corr = float(valid_data.select(\n",
    "                    pl.corr(feature, 'responder_6')\n",
    "                ).item())\n",
    "                \n",
    "                # Get regime-specific correlations\n",
    "                regime_corrs = valid_data.group_by('volatility_regime').agg([\n",
    "                    pl.corr(feature, 'responder_6').alias('correlation')\n",
    "                ])\n",
    "                \n",
    "                regime_dict = {\n",
    "                    str(row[0]): float(row[1]) \n",
    "                    for row in regime_corrs.iter_rows()\n",
    "                }\n",
    "                \n",
    "                if abs(overall_corr) > 0.05:  # Store only meaningful correlations\n",
    "                    importance_stats[feature] = {\n",
    "                        'overall_correlation': overall_corr,\n",
    "                        'regime_correlations': regime_dict\n",
    "                    }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {feature}: {e}\")\n",
    "        \n",
    "        return importance_stats\n",
    "    \n",
    "    def create_all_features(self) -> Tuple[pl.DataFrame, Dict]:\n",
    "        \"\"\"Create all enhanced features with staged execution\"\"\"\n",
    "        try:\n",
    "            print(\"\\nStage 1: Creating base features...\")\n",
    "            df = self.create_base_features()\n",
    "            print(\"Base features created successfully\")\n",
    "            \n",
    "            print(\"\\nStage 2: Detecting market regimes...\")\n",
    "            df = self.detect_regimes(df)\n",
    "            print(\"Regime detection completed\")\n",
    "            \n",
    "            print(\"\\nStage 3: Creating regime-aware features...\")\n",
    "            df = self.create_regime_features(df)\n",
    "            print(\"Regime features created\")\n",
    "            \n",
    "            print(\"\\nStage 4: Creating symbol-specific features...\")\n",
    "            df = self.create_symbol_features(df)\n",
    "            print(\"Symbol features created\")\n",
    "            \n",
    "            # Analyze new features\n",
    "            original_cols = set(self.df.columns)\n",
    "            new_features = [col for col in df.columns if col not in original_cols]\n",
    "            \n",
    "            print(f\"\\nCreated {len(new_features)} new features.\")\n",
    "            importance_stats = self.analyze_importance(df, new_features)\n",
    "            \n",
    "            return df, importance_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature creation pipeline: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data...\")\n",
    "    df = pl.read_parquet(\"../data/processed/engineered_features.parquet\")\n",
    "\n",
    "    print(\"\\nInitializing feature engineering...\")\n",
    "    engineer = EnhancedFeatureEngineer(df)\n",
    "    enhanced_df, importance_stats = engineer.create_all_features()\n",
    "\n",
    "    print(\"\\nFeature Importance Analysis:\")\n",
    "    for feature, stats in importance_stats.items():\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"- Overall correlation: {stats['overall_correlation']:.4f}\")\n",
    "        print(\"- Regime-specific correlations:\")\n",
    "        for regime, corr in stats['regime_correlations'].items():\n",
    "            print(f\"  * {regime}: {corr:.4f}\")\n",
    "\n",
    "    print(\"\\nSaving enhanced features...\")\n",
    "    enhanced_df.write_parquet(\"../data/processed/enhanced_features.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
